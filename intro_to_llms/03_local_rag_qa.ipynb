{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kUZtDdEHhm9C",
   "metadata": {
    "id": "kUZtDdEHhm9C"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade vllm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KtLt51iKhpXT",
   "metadata": {
    "id": "KtLt51iKhpXT"
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community pypdf sentence-transformers chromadb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c83617ea9b81d",
   "metadata": {},
   "source": [
    "# RAG: Retrieval Augmented Generation\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/juanhuguet/intro_to_llms/blob/main/intro_to_llms/03_local_rag_qa.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "![](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd07c8c5e1a90",
   "metadata": {},
   "source": [
    "![](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66ada1-c238-4d23-b86a-2b31efb099d1",
   "metadata": {
    "id": "4c66ada1-c238-4d23-b86a-2b31efb099d1"
   },
   "source": [
    "# Importamos las funcionalidades de langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e5348-6663-4308-ba9f-6dd646f824cc",
   "metadata": {
    "id": "420e5348-6663-4308-ba9f-6dd646f824cc"
   },
   "outputs": [],
   "source": [
    "from langchain_core.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms.vllm import VLLM\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5594f8a-19e4-4cc9-88aa-2518a97d211e",
   "metadata": {
    "id": "a5594f8a-19e4-4cc9-88aa-2518a97d211e"
   },
   "source": [
    "Seteamos las variables de entorno con la api key de openAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef7c94-d67f-4ff8-b133-52c462264c52",
   "metadata": {
    "id": "b1ef7c94-d67f-4ff8-b133-52c462264c52"
   },
   "source": [
    "## Preprocesamos el pdf\n",
    "\n",
    "PyPDF loader se encarga automáticamente de leer y extraer el texto.\n",
    "\n",
    "El splitter, crea `chunks` a partir del texto de aprox. 1000 caracteres y un overlap de 200 para no perder info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5550a4cc-e772-444c-bc50-ebbc0653bec4",
   "metadata": {
    "id": "5550a4cc-e772-444c-bc50-ebbc0653bec4"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data/insurance_policy_example.pdf\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=..., chunk_overlap=...)\n",
    "\n",
    "chunks = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eb8551-0952-42ba-ad6f-a697be3ec6b9",
   "metadata": {
    "id": "03eb8551-0952-42ba-ad6f-a697be3ec6b9"
   },
   "source": [
    "### Cargamos el documento en la base de datos vectorial.\n",
    "\n",
    "Usamos chroma, que esta en memoría y no requiere setup, y los embeddings a partir de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eb64a9-6b1d-4209-92e6-2b9494660b12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "82eb64a9-6b1d-4209-92e6-2b9494660b12",
    "outputId": "69f170f2-fdc0-4ce1-be2f-0fac33f43aa9"
   },
   "outputs": [],
   "source": [
    "embedder = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f940f555-dbfe-4de0-8fc8-386bcbbf4734",
   "metadata": {
    "id": "f940f555-dbfe-4de0-8fc8-386bcbbf4734"
   },
   "outputs": [],
   "source": [
    "# populamos la base de datos con los chunks, y los embedings\n",
    "docsearch = Chroma.from_documents(chunks, embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb606b-45f3-4b2a-873a-7cfd35580296",
   "metadata": {
    "id": "a2bb606b-45f3-4b2a-873a-7cfd35580296"
   },
   "source": [
    "### Creamos el `retriever`.\n",
    "\n",
    "Esta pieza permite dada una query, calcular su embedding y recuperar los  k chunks más relevantes para formar el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a16aab6-b404-405e-9c4e-802acb229241",
   "metadata": {
    "id": "7a16aab6-b404-405e-9c4e-802acb229241"
   },
   "outputs": [],
   "source": [
    "retriever=docsearch.as_retriever(search_type=\"mmr\", fetch_k=..., k=..., return_source_documents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0350d0-545a-45f5-aa47-5c23682ab984",
   "metadata": {
    "id": "1b0350d0-545a-45f5-aa47-5c23682ab984"
   },
   "source": [
    "### Creamos el reader\n",
    "\n",
    "Una vez recuperamos el contexto, lo pasaremos a un llm. Usaremos gpt-3.5-turbo, que tiene un buen ratio performance/coste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdbda8-b03e-4c5d-93e2-2323817836b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17fdbda8-b03e-4c5d-93e2-2323817836b4",
    "outputId": "ea87d57b-f813-4a1d-b822-19973ced9c68"
   },
   "outputs": [],
   "source": [
    "reader = VLLM(\n",
    "    model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=1000,\n",
    "    top_p=0.95,\n",
    "    stop=[\"\\n\\n\"],\n",
    "    temperature=0.3,\n",
    "    vllm_kwargs={\"quantization\": \"awq\",\n",
    "                 \"max_model_len\": 10000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eaab4d-5d4e-470e-a1cf-2e84419266e2",
   "metadata": {
    "id": "29eaab4d-5d4e-470e-a1cf-2e84419266e2"
   },
   "source": [
    "## Creamos el prompt\n",
    "\n",
    "Para poder controlar el comportamiento del modelo, creamos un prompt para dar instrucciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd98e4-c1ae-48d6-a610-0f7e58f76979",
   "metadata": {
    "id": "0acd98e4-c1ae-48d6-a610-0f7e58f76979"
   },
   "outputs": [],
   "source": [
    "prompt_template = \\\n",
    "\"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, don't try to make up an answer and answer `not-in-text`\n",
    "Answer in english only.\n",
    "\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04850e27-6328-4071-bf45-38fa916cfd0d",
   "metadata": {
    "id": "04850e27-6328-4071-bf45-38fa916cfd0d"
   },
   "source": [
    "## Creamos la cadena de QA\n",
    "\n",
    "Usamos el wrapper de langchain que se ocupa de orquestar el flujo de dato:\n",
    "\n",
    "`query` -> `embedding` -> `retrieve context` -> `prompt completion` -> answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800838b-d140-422f-a797-886062d95c5f",
   "metadata": {
    "id": "8800838b-d140-422f-a797-886062d95c5f"
   },
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=reader,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=retriever,\n",
    "                                 chain_type_kwargs={\"prompt\": PROMPT},\n",
    "                                 return_source_documents=True\n",
    "\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0db433-1227-4855-a581-ee66a7a033f3",
   "metadata": {
    "id": "ce0db433-1227-4855-a581-ee66a7a033f3"
   },
   "source": [
    "## Prueba del sistema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57cf29a-a4ec-4364-bacc-3c9eb611d49c",
   "metadata": {},
   "source": [
    "Vamos a ver como responde a la siguiente pregunta:\n",
    "\n",
    "What are the issues that are not covered by the insurance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a920c5-caad-4e22-ad99-75f099950ff2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30a920c5-caad-4e22-ad99-75f099950ff2",
    "outputId": "b162085c-abc0-48ca-e2dd-2ba084c7fbc0"
   },
   "outputs": [],
   "source": [
    "results = qa(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4614af2d-b147-49c9-97af-baad003e5434",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4614af2d-b147-49c9-97af-baad003e5434",
    "outputId": "d4e6c6d2-5f72-4041-805e-ba9c3b326012"
   },
   "outputs": [],
   "source": [
    "print(results[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bda86ed-9bd1-43ba-a3c2-08a9e99eb087",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bda86ed-9bd1-43ba-a3c2-08a9e99eb087",
    "outputId": "ed69b415-08ad-42bc-f3c2-ad581dcbf072"
   },
   "outputs": [],
   "source": [
    "for  in results[\"source_documents\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_n02Zcm2lR-x",
   "metadata": {
    "id": "_n02Zcm2lR-x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
