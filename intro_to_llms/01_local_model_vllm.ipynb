{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGj64hBeeCv-"
   },
   "source": [
    "# Large Language Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/juanhuguet/intro_to_llms/blob/main/intro_to_llms/01_local_model_vllm.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Large Language Models (LLMs) demonstrate impressive capabilities in handling textual data.\n",
    "\n",
    "One of their key features is the emergence of abilities that enable the use of a generalist model for a wide array of complex tasks. Previously, these tasks required extensive expertise in computational language modeling. Some of these tasks include:\n",
    "\n",
    "- Translation\n",
    "- Summarization\n",
    "- Entity extraction\n",
    "\n",
    "Additionally, LLMs simplify processes like text classification through their one-shot and few-shot learning capabilities.\n",
    "\n",
    "In this first lesson, we will explore how to utilize open-source LLMs in a local setupâ€”i.e., without relying on services from providers like OpenAI. We'll cover several tasks mentioned above to help you gain practical experience with these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zKeB9rtjBnh"
   },
   "source": [
    "## Setting up our notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Co7E0CbDk1wh"
   },
   "source": [
    "### Select the apropriate runtime\n",
    "Large Language Models (LLMs) require significant computational resources. To effectively use them, access to GPUs is essential. Google Colab offers GPUs for free in its basic tier, which makes it a valuable resource for those without their own powerful hardware. To utilize this, simply go to the \"Runtime\" menu in Google Colab and select the T4 GPU as your runtime environment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3g-nUJ_MkewU"
   },
   "source": [
    "### Install the needed libraries\n",
    "Once we have selected the apropriate runtime, install the needed libraries to go through this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6hQ2DnMjpsf"
   },
   "source": [
    "### vLLM\n",
    "\n",
    "![vllm](https://docs.vllm.ai/en/latest/_images/vllm-logo-text-light.png)\n",
    "\n",
    "It a fast and easy-to-use library for LLM inference and serving that offers state-of-the-art serving throughput with capabilities to run batch requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yoaa9llcC2M"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade vllm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6epSS8z-kIAi"
   },
   "source": [
    "#### Lanchain\n",
    "\n",
    "![langchain](https://python.langchain.com/img/brand/wordmark.png)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x-2NqNGGeKEd"
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain_community -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVOSCxNxkjYx"
   },
   "source": [
    "# Run your local LLM\n",
    "\n",
    "To interact with your local LLM by asking questions, you first need to initialize it.\n",
    "\n",
    "We'll use LangChain as a high-level wrapper and vLLM as the serving engine. This setup allows us to seamlessly manage and query the model.\n",
    "\n",
    "Wer are going to use the model:\n",
    "\n",
    "`\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ngc_Okcgdat8",
    "outputId": "18bc230e-c663-4f6a-8ed5-bc1572a49b35"
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import ...\n",
    "\n",
    "llm = ...(\n",
    "    model=...,\n",
    "    trust_remote_code=...,  # mandatory for hf models\n",
    "    max_new_tokens=...,\n",
    "    top_p=...,\n",
    "    temperature=...,\n",
    "    vllm_kwargs={\"quantization\": \"awq\",\n",
    "                 \"max_model_len\": 10000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avkCYTPalOfK"
   },
   "source": [
    "Now let's run our first query..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KI4V3a_5eqt8",
    "outputId": "9a714370-3a8d-438e-bce4-aacf8559349a"
   },
   "outputs": [],
   "source": [
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wbZyEYxJnCDk"
   },
   "source": [
    "Congrats ! you have run your first query in a local LLM !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsSJ66D1nIru"
   },
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "## Prompt Templating\n",
    "\n",
    "Special Tokens in LLMs: Special tokens are essential for distinguishing between user inputs and model responses in conversational AI. These tokens, like `[INST]` for user messages and `<s>` for new instructions in the Mistral model, help maintain clear communication flows. Since LLMs only output text, using special tokens cleverly structures the conversation, ensuring the model correctly interprets and responds to each part of the dialogue.\n",
    "\n",
    "Why Use Templates? Templates are vital because they ensure inputs are formatted correctly, facilitating accurate and relevant responses from the model. They guide the model in processing and responding to the input effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExSgJLElgm4i"
   },
   "outputs": [],
   "source": [
    "prompt = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqdTqf_3fw9d"
   },
   "outputs": [],
   "source": [
    "prompt_template=f\"\"\"<s>[INST]{prompt}[/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skh5lLVngrTT",
    "outputId": "227f27eb-adc5-45bd-c729-f8d223d56ba2"
   },
   "outputs": [],
   "source": [
    "response = llm.invoke(prompt_template, stop=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TzC3A9jhXw0",
    "outputId": "f0ef0ee6-632d-413a-dcfd-4f7a8e439e4b"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRfd1QHdsER6"
   },
   "source": [
    "## Prompt Structure\n",
    "\n",
    "![](https://cdn.sanity.io/images/vr8gru94/production/6c9703965f770d56b19d5d0adc7ad76ac2d28412-3720x1552.png)\n",
    "\n",
    "Creating a well-structured prompt for a Large Language Model (LLM) can significantly improve the accuracy and relevance of the model's outputs. Here's a basic structure that you can follow to create an effective LLM prompt:\n",
    "\n",
    "1. **Introduction/Context**: Provide any necessary background information that the model needs to understand the context of the task. This could include the nature of the task, specific details relevant to the query, or any constraints that should guide the model's responses.\n",
    "\n",
    "2. **Clear Instruction**: Clearly state what you need from the model. Whether it's generating text, answering a question, summarizing information, or performing an analysis, the instruction should be unambiguous and direct.\n",
    "\n",
    "3. **Specific Details/Parameters**: If there are particular details or parameters that the model needs to consider, mention these explicitly. This could include the tone of the response, the target audience, length constraints, or specific points that must be covered in the response.\n",
    "\n",
    "4. **Examples (Optional)**: For complex tasks, providing an example of the desired output can guide the model more effectively. This is especially useful in \"few-shot\" learning scenarios where the model uses the example as a template for generating its response.\n",
    "\n",
    "5. **Closure (Optional)**: Sum up or clarify any final points that might help the model focus its generation. This can be particularly useful for open-ended tasks to narrow down the scope of possible responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVCrdvyFp6iO"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkIdIaZlswlQ"
   },
   "outputs": [],
   "source": [
    "introduction_context = \"Given an ingredient, i am looking to cook a meal using it as the main ingredient.\\n\"\n",
    "\n",
    "clear_instruction = \"Please create a recipe that uses {ingredient}.\\n\"\n",
    "\n",
    "parameters = \\\n",
    "\"\"\"\n",
    "The recipe should be vegetarian.\n",
    "It should serve four people.\n",
    "Include a list of all necessary ingredients.\n",
    "Provide step-by-step cooking instructions.\n",
    "The total cooking time should not exceed one hour.\n",
    "\"\"\"\n",
    "\n",
    "examples = \"For example, if the input are chickpeas, you might suggest a chickpea curry with vegetables, detailing the spices and preparation steps.\\n\"\n",
    "\n",
    "closure = \"Ensure the recipe is simple and can be prepared with common kitchen tools.\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5s7BnE8r96q"
   },
   "outputs": [],
   "source": [
    "template = introduction_context + clear_instruction + parameters + examples + closure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "fCmjClFStfOY",
    "outputId": "42e0fde9-793d-41fa-856b-38efa982644e"
   },
   "outputs": [],
   "source": [
    "# print the template text\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNK-C3H1tgeN"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=..., # as we have ingredient in the prompt, let's pass the ingredient as input var\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW-BJWBStnlK",
    "outputId": "48eee0c3-9921-4746-ee32-770e653e66cf"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    prompt_template.format(ingredient=...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09HePaRAtt7H",
    "outputId": "7460d43c-a71e-4f0e-af23-5bf663c0c315"
   },
   "outputs": [],
   "source": [
    "prompt_template.invoke(...) # insert directly an ingredient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TY1mu7ymwlrC"
   },
   "source": [
    "Now, let's chain it to the LLM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ve90JamxwqYL",
    "outputId": "d6ae4f0a-6618-4158-a97c-11c11795347c"
   },
   "outputs": [],
   "source": [
    "input_ingredient = \"lentils\"\n",
    "\n",
    "prompt = prompt_template.invoke(input_ingredient)\n",
    "\n",
    "response = llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yO4dTWqKxBOZ",
    "outputId": "a0393b89-1dba-4714-d4bc-8afdebd76a3b"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boEIzm1lxFFt"
   },
   "source": [
    "### Langchain is for composability: LCEL\n",
    "\n",
    "Since we are using langhcain components, we can chain them to create a pipeline of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os7tjnC2uKMY"
   },
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yexKEInQur9u",
    "outputId": "d765d81a-5256-4883-a02c-37a00aa8b852"
   },
   "outputs": [],
   "source": [
    "response = chain.invoke({\"ingredient\": \"carrots\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCOMUALJuxAB",
    "outputId": "6b449c02-45e1-4814-c25b-f359675630c8"
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
