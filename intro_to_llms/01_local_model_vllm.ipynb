{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Large Language Models\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/juanhuguet/intro_to_llms/blob/main/intro_to_llms/01_local_model_vllm.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "Large Language Models (LLMs) demonstrate impressive capabilities in handling textual data.\n",
    "\n",
    "One of their key features is the emergence of abilities that enable the use of a generalist model for a wide array of complex tasks. Previously, these tasks required extensive expertise in computational language modeling. Some of these tasks include:\n",
    "\n",
    "- Translation\n",
    "- Summarization\n",
    "- Entity extraction\n",
    "\n",
    "Additionally, LLMs simplify processes like text classification through their one-shot and few-shot learning capabilities.\n",
    "\n",
    "In this first lesson, we will explore how to utilize open-source LLMs in a local setup—i.e., without relying on services from providers like OpenAI. We'll cover several tasks mentioned above to help you gain practical experience with these models."
   ],
   "metadata": {
    "id": "zGj64hBeeCv-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up our notebook"
   ],
   "metadata": {
    "id": "_zKeB9rtjBnh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select the apropriate runtime\n",
    "Large Language Models (LLMs) require significant computational resources. To effectively use them, access to GPUs is essential. Google Colab offers GPUs for free in its basic tier, which makes it a valuable resource for those without their own powerful hardware. To utilize this, simply go to the \"Runtime\" menu in Google Colab and select the T4 GPU as your runtime environment.\n",
    "\n"
   ],
   "metadata": {
    "id": "Co7E0CbDk1wh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Install the needed libraries\n",
    "Once we have selected the apropriate runtime, install the needed libraries to go through this exercise"
   ],
   "metadata": {
    "id": "3g-nUJ_MkewU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### vLLM\n",
    "\n",
    "![vllm](https://docs.vllm.ai/en/latest/_images/vllm-logo-text-light.png)\n",
    "\n",
    "It a fast and easy-to-use library for LLM inference and serving that offers state-of-the-art serving throughput with capabilities to run batch requests"
   ],
   "metadata": {
    "id": "_6hQ2DnMjpsf"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install --upgrade vllm -q"
   ],
   "metadata": {
    "id": "9yoaa9llcC2M"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lanchain\n",
    "\n",
    "![langchain](https://python.langchain.com/img/brand/wordmark.png)\n",
    "\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs)."
   ],
   "metadata": {
    "id": "6epSS8z-kIAi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install langchain langchain_community -q"
   ],
   "metadata": {
    "id": "x-2NqNGGeKEd"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run your local LLM\n",
    "\n",
    "To interact with your local LLM by asking questions, you first need to initialize it.\n",
    "\n",
    "We'll use LangChain as a high-level wrapper and vLLM as the serving engine. This setup allows us to seamlessly manage and query the model."
   ],
   "metadata": {
    "id": "LVOSCxNxkjYx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.llms import VLLM\n",
    "\n",
    "llm = VLLM(\n",
    "    model=\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\",\n",
    "    trust_remote_code=True,  # mandatory for hf models\n",
    "    max_new_tokens=1000,\n",
    "    top_p=0.95,\n",
    "    temperature=0.3,\n",
    "    vllm_kwargs={\"quantization\": \"awq\",\n",
    "                 \"max_model_len\": 10000},\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ngc_Okcgdat8",
    "outputId": "18bc230e-c663-4f6a-8ed5-bc1572a49b35"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING 05-05 17:13:01 config.py:205] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-05 17:13:01 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', speculative_config=None, tokenizer='TheBloke/Mistral-7B-Instruct-v0.2-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=TheBloke/Mistral-7B-Instruct-v0.2-AWQ)\n",
      "INFO 05-05 17:13:01 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 05-05 17:13:02 selector.py:69] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 05-05 17:13:02 selector.py:32] Using XFormers backend.\n",
      "INFO 05-05 17:13:04 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 05-05 17:13:06 model_runner.py:175] Loading model weights took 3.8814 GB\n",
      "INFO 05-05 17:13:12 gpu_executor.py:114] # GPU blocks: 3494, # CPU blocks: 2048\n",
      "INFO 05-05 17:13:14 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 05-05 17:13:14 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 05-05 17:13:30 model_runner.py:1017] Graph capturing finished in 16 secs.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's run our first query..."
   ],
   "metadata": {
    "id": "avkCYTPalOfK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(llm.invoke(\"What is the capital of france ? answer just in one word\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KI4V3a_5eqt8",
    "outputId": "9a714370-3a8d-438e-bce4-aacf8559349a"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.74s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " : Paris\n",
      "\n",
      "Paris is indeed the capital city of France. It is a major global city and a significant cultural, artistic, and commercial center; home to many world-renowned museums, monuments, and attractions. Paris is also known for its fashion, gastronomy, and rich history. It is the most populous city in France and the European Union, with a population of over 10 million people in the metropolitan area. Paris has been a major center of art, science, and culture since the 19th century, and it continues to be a global hub for innovation and creativity today.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Congrats ! you have run your first query in a local LLM !"
   ],
   "metadata": {
    "id": "wbZyEYxJnCDk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prompt Engineering\n",
    "\n",
    "## Prompt Templating\n",
    "\n",
    "Special Tokens in LLMs: Special tokens are essential for distinguishing between user inputs and model responses in conversational AI. These tokens, like `[INST]` for user messages and `<s>` for new instructions in the Mistral model, help maintain clear communication flows. Since LLMs only output text, using special tokens cleverly structures the conversation, ensuring the model correctly interprets and responds to each part of the dialogue.\n",
    "\n",
    "Why Use Templates? Templates are vital because they ensure inputs are formatted correctly, facilitating accurate and relevant responses from the model. They guide the model in processing and responding to the input effectively.\n"
   ],
   "metadata": {
    "id": "PsSJ66D1nIru"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"You are a helpful analyst. I want you to answer the next question with just the answer in one word only: What is the capital of France?\""
   ],
   "metadata": {
    "id": "ExSgJLElgm4i"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_template=f\"\"\"<s>[INST]{prompt}[/INST]\"\"\""
   ],
   "metadata": {
    "id": "nqdTqf_3fw9d"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = llm.invoke(prompt_template, stop=[\"\\n\"])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "skh5lLVngrTT",
    "outputId": "227f27eb-adc5-45bd-c729-f8d223d56ba2"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  4.17it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TzC3A9jhXw0",
    "outputId": "f0ef0ee6-632d-413a-dcfd-4f7a8e439e4b"
   },
   "execution_count": 23,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Paris.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Structure\n",
    "\n",
    "![](https://cdn.sanity.io/images/vr8gru94/production/6c9703965f770d56b19d5d0adc7ad76ac2d28412-3720x1552.png)\n",
    "\n",
    "Creating a well-structured prompt for a Large Language Model (LLM) can significantly improve the accuracy and relevance of the model's outputs. Here's a basic structure that you can follow to create an effective LLM prompt:\n",
    "\n",
    "1. **Introduction/Context**: Provide any necessary background information that the model needs to understand the context of the task. This could include the nature of the task, specific details relevant to the query, or any constraints that should guide the model's responses.\n",
    "\n",
    "2. **Clear Instruction**: Clearly state what you need from the model. Whether it's generating text, answering a question, summarizing information, or performing an analysis, the instruction should be unambiguous and direct.\n",
    "\n",
    "3. **Specific Details/Parameters**: If there are particular details or parameters that the model needs to consider, mention these explicitly. This could include the tone of the response, the target audience, length constraints, or specific points that must be covered in the response.\n",
    "\n",
    "4. **Examples (Optional)**: For complex tasks, providing an example of the desired output can guide the model more effectively. This is especially useful in \"few-shot\" learning scenarios where the model uses the example as a template for generating its response.\n",
    "\n",
    "5. **Closure (Optional)**: Sum up or clarify any final points that might help the model focus its generation. This can be particularly useful for open-ended tasks to narrow down the scope of possible responses."
   ],
   "metadata": {
    "id": "nRfd1QHdsER6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate"
   ],
   "metadata": {
    "id": "bVCrdvyFp6iO"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "introduction_context = \"Given an ingredient, i am looking to cook a meal using it as the main ingredient.\\n\"\n",
    "\n",
    "clear_instruction = \"Please create a recipe that uses {ingredient}.\\n\"\n",
    "\n",
    "parameters = \\\n",
    "\"\"\"\n",
    "The recipe should be vegetarian.\n",
    "It should serve four people.\n",
    "Include a list of all necessary ingredients.\n",
    "Provide step-by-step cooking instructions.\n",
    "The total cooking time should not exceed one hour.\n",
    "\"\"\"\n",
    "\n",
    "examples = \"For example, if the input are chickpeas, you might suggest a chickpea curry with vegetables, detailing the spices and preparation steps.\\n\"\n",
    "\n",
    "closure = \"Ensure the recipe is simple and can be prepared with common kitchen tools.\\n\"\n"
   ],
   "metadata": {
    "id": "CkIdIaZlswlQ"
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "template = introduction_context + clear_instruction + parameters + examples + closure"
   ],
   "metadata": {
    "id": "p5s7BnE8r96q"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "template"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "fCmjClFStfOY",
    "outputId": "42e0fde9-793d-41fa-856b-38efa982644e"
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Given an ingredient, i am looking to cook a meal using it as the main ingredient.\\nPlease create a recipe that uses {ingredient}.\\n\\nThe recipe should be vegetarian.\\nIt should serve four people.\\nInclude a list of all necessary ingredients.\\nProvide step-by-step cooking instructions.\\nThe total cooking time should not exceed one hour.\\nFor example, if the input are chickpeas, you might suggest a chickpea curry with vegetables, detailing the spices and preparation steps.\\nEnsure the recipe is simple and can be prepared with common kitchen tools.\\n'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 27
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"ingredient\"],\n",
    "    template=template\n",
    ")"
   ],
   "metadata": {
    "id": "qNK-C3H1tgeN"
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\n",
    "    prompt_template.format(ingredient=\"chicken\")\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZW-BJWBStnlK",
    "outputId": "48eee0c3-9921-4746-ee32-770e653e66cf"
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Given an ingredient, i am looking to cook a meal using it as the main ingredient.\n",
      "Please create a recipe that uses chicken.\n",
      "\n",
      "The recipe should be vegetarian.\n",
      "It should serve four people.\n",
      "Include a list of all necessary ingredients.\n",
      "Provide step-by-step cooking instructions.\n",
      "The total cooking time should not exceed one hour.\n",
      "For example, if the input are chickpeas, you might suggest a chickpea curry with vegetables, detailing the spices and preparation steps.\n",
      "Ensure the recipe is simple and can be prepared with common kitchen tools.\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_template.invoke(\"chicken\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09HePaRAtt7H",
    "outputId": "7460d43c-a71e-4f0e-af23-5bf663c0c315"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StringPromptValue(text='Given an ingredient, i am looking to cook a meal using it as the main ingredient.\\nPlease create a recipe that uses chicken.\\n\\nThe recipe should be vegetarian.\\nIt should serve four people.\\nInclude a list of all necessary ingredients.\\nProvide step-by-step cooking instructions.\\nThe total cooking time should not exceed one hour.\\nFor example, if the input are chickpeas, you might suggest a chickpea curry with vegetables, detailing the spices and preparation steps.\\nEnsure the recipe is simple and can be prepared with common kitchen tools.\\n')"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's chain it to the LLM..."
   ],
   "metadata": {
    "id": "TY1mu7ymwlrC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input = \"lentils\"\n",
    "\n",
    "prompt = prompt_template.invoke(input)\n",
    "\n",
    "response = llm.invoke(prompt)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ve90JamxwqYL",
    "outputId": "d6ae4f0a-6618-4158-a97c-11c11795347c"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.85s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yO4dTWqKxBOZ",
    "outputId": "a0393b89-1dba-4714-d4bc-8afdebd76a3b"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Recipe: Lentil Soup\n",
      "\n",
      "Ingredients:\n",
      "- 2 cups (1 pound) dried brown or green lentils, rinsed and drained\n",
      "- 1 onion, chopped\n",
      "- 2 carrots, chopped\n",
      "- 2 celery stalks, chopped\n",
      "- 1 red bell pepper, chopped\n",
      "- 1 green bell pepper, chopped\n",
      "- 4 cloves garlic, minced\n",
      "- 1 (14.5 oz) can diced tomatoes\n",
      "- 1 (15 oz) can red kidney beans, drained and rinsed\n",
      "- 1 (15 oz) can black-eyed peas, drained and rinsed\n",
      "- 1 (15 oz) can chickpeas, drained and rinsed\n",
      "- 1 (15 oz) can corn, drained\n",
      "- 6 cups vegetable broth\n",
      "- 1 tbsp olive oil\n",
      "- 1 tbsp cumin\n",
      "- 1 tbsp paprika\n",
      "- 1 tbsp chili powder\n",
      "- Salt and pepper to taste\n",
      "- Optional: 1 tbsp lemon juice\n",
      "\n",
      "Instructions:\n",
      "1. Heat olive oil in a large pot over medium heat. Add onion, carrots, celery, bell peppers, and garlic, and cook until softened, about 5 minutes.\n",
      "2. Stir in cumin, paprika, and chili powder, and cook for 1 minute.\n",
      "3. Add rinsed and drained lentils, diced tomatoes (with their juice), kidney beans, black-eyed peas, chickpeas, corn, and vegetable broth.\n",
      "4. Bring the soup to a boil, then reduce heat and let it simmer for about 30 minutes, or until lentils are tender.\n",
      "5. Season with salt, pepper, and optional lemon juice to taste.\n",
      "6. Serve hot, garnished with chopped cilantro or parsley, if desired.\n",
      "\n",
      "This vegetarian lentil soup serves four people and can be prepared within one hour. It is a simple and delicious meal that can be made with common kitchen tools. Enjoy!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langchain is for composability: LCEL\n",
    "\n",
    "Since we are using langhcain components, we can chain them to create a pipeline of tasks"
   ],
   "metadata": {
    "id": "boEIzm1lxFFt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "chain = prompt_template | llm"
   ],
   "metadata": {
    "id": "os7tjnC2uKMY"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "response = chain.invoke({\"ingredient\": \"carrots\"})"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yexKEInQur9u",
    "outputId": "d765d81a-5256-4883-a02c-37a00aa8b852"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.19s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(response)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCOMUALJuxAB",
    "outputId": "6b449c02-45e1-4814-c25b-f359675630c8"
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Here's a simple and delicious vegetarian recipe using carrots as the main ingredient. This dish is called \"Glazed Carrots with Ginger and Honey.\"\n",
      "\n",
      "Ingredients:\n",
      "- 1 lb (450g) carrots, sliced into rounds\n",
      "- 2 tbsp (30ml) vegetable oil\n",
      "- 1 tbsp (15g) unsalted butter\n",
      "- 1 tbsp (15g) grated ginger\n",
      "- 1/4 cup (60g) honey\n",
      "- 1/4 cup (60ml) water\n",
      "- Salt, to taste\n",
      "- 1 tbsp (15g) chopped parsley, for garnish\n",
      "\n",
      "Instructions:\n",
      "1. Heat vegetable oil in a large skillet over medium heat. Add sliced carrots and cook for 5 minutes, stirring occasionally.\n",
      "\n",
      "2. Melt butter in the skillet and add grated ginger. Cook for 1 minute until fragrant.\n",
      "\n",
      "3. In a small bowl, whisk together honey and water.\n",
      "\n",
      "4. Pour the honey-water mixture over the carrots and bring to a simmer. Cook for 15 minutes, stirring occasionally, until the carrots are tender and the glaze has thickened.\n",
      "\n",
      "5. Season with salt to taste.\n",
      "\n",
      "6. Remove from heat and garnish with chopped parsley.\n",
      "\n",
      "7. Serve immediately and enjoy your Glazed Carrots with Ginger and Honey!\n",
      "\n",
      "Total cooking time: Approximately 25 minutes for cooking the carrots, plus 15 minutes for the glaze, making a total of 40 minutes. This recipe serves four people.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "-vfbvrUFxRGp"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
